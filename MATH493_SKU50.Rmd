---
title: "Top 50 SKU's EDA and Preliminary Modeling Steps"
author: "Carson Slater"
date: '2022-09-10'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```
```{r, include=FALSE}
# Loading packages
library(tidymodels)
library(stringr)
library(janitor)
library(glmnet)
library(lubridate)
library(knitr)
library(mosaic)
library(doParallel)
```

```{r, include=FALSE}
# For parallel processing
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
```
### The Background

This is the **third** .Rmd file dedicated to EDA for my poster project in the Fall of 2022 aimed at forecasting demand for particular FMCS data. FMCG's are fast-moving, consumer goods, meaning that lots of research can be done here that is market basket analysis! A basket is a consumer's purchases at a given time.

#### The Problem
Expectation of demand largely influences the movement of consumer goods within a free market economic system, and managing the flow by expectation and optimization can help reduce cost and best steward FMCG's. We hope to build a model to forecast SKU demand for a particular Indonesian distribution center (DC). We selected their top 50 most popular SKU's (by total volume) over a given series of time, and aim to build model that can forecast up to a month in advance optimal amount of these goods they will need to have in stock during that period.

### Importing the Data### Loading the Full Data
```{r, include=TRUE}
url <- "https://raw.githubusercontent.com/carsonslater/mentored_research2022/Main/new_baskets_full.csv"

full <- read.csv(url)
```

### Tidying the Data
```{r}
# Finding Percentage of the Missing Data for each column.
colMeans(is.na(full))*100
# There is a very small proportion of missing data in these data 

full <- full %>% mutate(id = as.factor(id), 
                        order_id = as.factor(order_id), 
                        merchant_id = as.factor(merchant_id), 
                        sku_id = as.factor(sku_id), 
                        top_cat_id = as.factor(top_cat_id), 
                        sub_cat_id = as.factor(sub_cat_id))

# Cleaning data so that R can read the time stamp
full$placed_at = substr(full$placed_at, 1, nchar(full$placed_at)-4)

# Changing the placed_by into a POSIXct variable type
full$placed_at = as.POSIXct(full$placed_at)

# Finding NA's
full %>% filter(is.na(full$top_cat_id))

# Removing the 11 NA's
full <- full %>% filter(!is.na(full$top_cat_id))


# Creating more columns with more date-specific information
full <- full %>% mutate(year = format(full$placed_at, "%Y"),
                        month = format(full$placed_at, "%m"),
                        day = format(full$placed_at, "%d"),
                        hour = format(full$placed_at, "%H"),
                        minute = format(full$placed_at, "%M"),
                        second = format(full$placed_at, "%S"),
                        yday = yday(full$placed_at),
                        wday = wday(full$placed_at),
                        yweek = week(full$placed_at))

# Creating factor variables for dates and times
full <- full %>% mutate(year = as.factor(year),
                        month = as.factor(month),
                        day = as.factor(day),
                        hour = as.factor(hour),
                        minute = as.factor(minute),
                        second = as.factor(second),
                        yday = as.factor(yday),
                        wday = as.factor(wday),
                        yweek = as.factor(yweek))

# Looking for duplicates orders
dupes <- get_dupes(full, order_id, placed_at, merchant_id, sku_id)

# Removing the 178 duplicate observations
full <- full %>% distinct(order_id, placed_at, merchant_id, sku_id, .keep_all = TRUE)
```

```{r, include=FALSE}
# For visualization purposes
library(RColorBrewer)
n <- 50
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]

col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, 
                           rownames(qual_col_pals)))

# The color schema I am using
pie(rep(1,n), col=sample(col_vector, n))
```

### Finding the Top 50 SKU's by Total Volume
```{r}
# Collecting the top 50 SKU's
qty_grp <- full %>% filter(!(order_id == 48674)) %>% # I filtered the outlier order
  group_by(sku_id) %>% 
  summarize(tot_qty = sum(qty)) %>% 
  arrange(desc(tot_qty)) %>% 
  slice(1:50)

# Creating a new data frame with only the top 50 SKU's by volume
full_50 <- full %>% filter(full$sku_id %in% qty_grp$sku_id)

head(full_50)

# Checking if there are only 50 unique SKU's in full_50
full_50 %>% summarize(count = n_distinct(sku_id))
```

#### Adjusted Questions
The prior two EDA `.Rmd` files contained a list of general exploratory questions. Here we have posed similar yet adjusted questions that aim to uncover more relevant information, since we have a narrowed goal and problem.

 - What kinds of goods are sold the most frequently and at the highest volume?
 - What are the average times between purchases for each good?
 - Can we detect any automated purchasing?
 - Which goods are sold at higher quantities?
 - How many total merchants are represented in this subset of the data.?
 - How frequent do these merchants purchase from this DC?
 - Were these merchants customers throughout the entire time these data were collected?
 - What did these merchants purchase? How much? At what price?
 - Does price fluctuate for each SKU throughout the time series? How much?
 - What are the most costly items and how does that correlate with time between the prior purchase?

#### These SKU's which are sold at the highest volume on average?
```{r}
first_25 <- head(qty_grp, 25)
last_25 <- tail(qty_grp, 25)

med_qty_grp <- full_50 %>% 
  group_by(sku_id) %>% 
  summarize(med_qty = median(qty)) %>% 
  arrange(desc(med_qty))

avg_qty_grp <- full_50 %>% 
  group_by(sku_id) %>% 
  summarize(avg_qty = mean(qty)) %>% 
  arrange(desc(avg_qty))
```

```{r}
# Plots
full_50 %>% filter(full_50$sku_id %in% first_25$sku_id) %>% 
  ggplot(aes(x = sku_id, y = qty, fill = sku_id)) +
  geom_boxplot(color = "black") + 
  ylim(0, 300) + scale_fill_manual(values = col_vector)

full_50 %>% filter(full_50$sku_id %in% last_25$sku_id) %>% 
  ggplot(aes(x = sku_id, y = qty, fill = sku_id)) +
  geom_boxplot(color = "black") + 
  ylim(0, 300) + scale_fill_manual(values = col_vector)
```

There appears to be a confounding instance, where SKU 277 has the highest average quantity per purchase, but there appears to only be one or two very high quantity orders for this particular good. Overall, SKU 1300 is leading the charge. In particular, we discovered two orders that we will count as outliers for our data.

#### What are the average times between purchases for each good?

