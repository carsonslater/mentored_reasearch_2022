---
title: "SKU_1300_ExpSmooth"
author: "Carson Slater"
date: '2022-09-17'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

```{r}
library(tidymodels)
library(tidyselect)
library(forecast)
library(lubridate)
library(zoo)
library(fpp2)
```

### Importing the Data
```{r, include=TRUE}
url <- "https://raw.githubusercontent.com/carsonslater/mentored_research2022/Main/new_baskets_full.csv"

full <- read.csv(url)
```

### Tidying the Data
```{r}
# Finding Percentage of the Missing Data for each column.
colMeans(is.na(full))*100
# There is a very small proportion of missing data in these data 

full <- full %>% mutate(id = as.factor(id), 
                        order_id = as.factor(order_id), 
                        merchant_id = as.factor(merchant_id), 
                        sku_id = as.factor(sku_id), 
                        top_cat_id = as.factor(top_cat_id), 
                        sub_cat_id = as.factor(sub_cat_id))

# Cleaning data so that R can read the time stamp
full$placed_at = substr(full$placed_at, 1, nchar(full$placed_at)-4)

# Changing the placed_by into a POSIXct variable type
full$placed_at = as.POSIXct(full$placed_at)

# Finding NA's
full %>% filter(is.na(full$top_cat_id))

# Removing the 11 NA's
full <- full %>% filter(!is.na(full$top_cat_id))


# Creating more columns with more date-specific information
full <- full %>% mutate(year = format(full$placed_at, "%Y"),
                        month = format(full$placed_at, "%m"),
                        day = format(full$placed_at, "%d"),
                        hour = format(full$placed_at, "%H"),
                        minute = format(full$placed_at, "%M"),
                        second = format(full$placed_at, "%S"),
                        yday = yday(full$placed_at),
                        wday = wday(full$placed_at),
                        yweek = week(full$placed_at))

# Creating factor variables for dates and times
full <- full %>% mutate(year = as.factor(year),
                        month = as.factor(month),
                        day = as.factor(day),
                        hour = as.factor(hour),
                        minute = as.factor(minute),
                        second = as.factor(second),
                        yday = as.factor(yday),
                        wday = as.factor(wday),
                        yweek = as.factor(yweek))

# Turning the timestamp into an actual date
full <- full %>% mutate(placed_at = as.Date(placed_at))

# Removing the 178 duplicate observations
full <- full %>% distinct(order_id, placed_at, merchant_id, sku_id, .keep_all = TRUE)
```

### Finding the Top 50 SKU's by Total Volume
```{r}
# Collecting the top 50 SKU's
qty_grp <- full %>% filter(!(order_id == 48674)) %>% # I filtered the outlier order
  group_by(sku_id) %>% 
  summarize(tot_qty = sum(qty), avg_qty = mean(qty), med_qty = median(qty)) %>% 
  mutate(diff = avg_qty - med_qty) %>%
  filter(diff < 100) %>%
  arrange(desc(tot_qty)) %>% 
  slice(1:50)

# Creating a new data frame with only the top 50 SKU's by volume
full_50 <- full %>% filter(full$sku_id %in% qty_grp$sku_id)

head(full_50)

# Checking if there are only 50 unique SKU's in full_50
full_50 %>% summarize(count = n_distinct(sku_id))
```

```{r}
no_1300 <- full_50 %>% filter(sku_id == 1300) %>% 
  select(placed_at, qty) %>% 
  rename(ds = placed_at, y = qty)%>% 
  group_by(ds) %>% 
  mutate(y = sum(y)) %>%
  unique() 

no_1300 %>% ggplot(aes(x = ds, y = y)) +
  geom_line()
```

### Create a Time Series Object
```{r}
no_1300 <- no_1300 %>%
  complete(ds = seq.Date(min(no_1300$ds), max(no_1300$ds), by="day")) %>% unique()

# Finally, a time series df
no_1300[is.na(no_1300)] <- 0

myts <- ts(no_1300$y,     # my data
           start = c(2022, as.numeric(format(no_1300$ds[1],"%j"))),
           end = c(2022, as.numeric(format(no_1300$ds[141],"%j"))),
           frequency = 141)
```

### Additional Model Experimentation
```{r}
startW <- as.numeric(strftime(head(no_1300$ds, 1), format = "%W"))
startD <- as.numeric(strftime(head(no_1300$ds, 1) + 1, format =" %w"))

print(ts(data = no_1300$y, frequency = 7, start = c(startW, startD)), calendar = T)

ts <- ts(data = no_1300$y, frequency = 7, start = c(startW, startD))
```


### Building a Benchmark Model
```{r}
############################
# With unconvential method
############################
snaive_fit <- snaive(myts) ## Standard Deviation is 5861.95

print(summary(snaive_fit))

autoplot(snaive_fit)

checkresiduals(snaive_fit)

############################
# With weekly method
############################
wkly_snaive_fit <- snaive(ts) ## Standard Deviation is 5861.95

print(summary(wkly_snaive_fit))

autoplot(wkly_snaive_fit)

checkresiduals(wkly_snaive_fit)

snaive_fcst <- forecast(wkly_snaive_fit, h = 16)
autoplot(snaive_fcst)
```

### With ETS
```{r}
############################
# With unconvential method
############################
fit_ets <- ets(myts)

print(summary(fit_ets))

autoplot(fit_ets)

checkresiduals(fit_ets)

############################
# With weekly method
############################

wkly_fit_ets <- ets(ts) # SD is 3222.337

print(summary(wkly_fit_ets))

autoplot(wkly_fit_ets)

checkresiduals(wkly_fit_ets)

# Essentially the same
```

### With ARIMA
```{r}
arima_fit <- auto.arima(ts, stepwise = FALSE, approximation = FALSE, trace = TRUE)

print(arima_fit)

plot(arima_fit)

checkresiduals(arima_fit)

arima_fcst <- forecast(arima_fit, h = 26)

autoplot(arima_fcst)
```

```{r}
fcst <- forecast(wkly_fit_ets, h = 365)
autoplot(fcst)

```

