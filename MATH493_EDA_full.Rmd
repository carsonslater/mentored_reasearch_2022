---
title: "SKU EDA (Full Data)"
author: "Carson Slater"
date: '2022-09-09'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# Loading packages
library(tidymodels)
library(stringr)
library(janitor)
library(glmnet)
library(lubridate)
library(knitr)
library(mosaic)
library(doParallel)
```

```{r, include=FALSE}
# For parallel processing
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
```

### The Background

This is the second .Rmd file dedicated to EDA for my poster project in the Fall of 2022 aimed at forecasting demand for particular FMCS data. FMCG's are fast-moving, consumer goods, meaning that lots of research can be done here that is market basket analysis! Basket is a consumer's purchases at a given time.

Unfortunately, this is not a market basket analysis project. Here are some of the questions that I have asked, some of which I have tapped into, but others I have yet to explore:

 - What are the busiest times of the year to make note of?
 - What kinds of goods are sold the most frequently?
 - What kinds of goods are sold at higher quantities?
 - What are the average quantities sold for each SKU?
 - Are any of the types of goods sold at higher rates during certain parts of the year?
 - Can we detect any automated purchasing?
 - How many total merchants is this distribution center (DC) servicing?
 - How frequent do these merchants purchase from this DC?
 - Were these merchants customers throughout the entire time these data were collected?
 - What did these merchants purchase? How much?
 
### New Questions I have asked.

The questions listed above are less pin-pointed and have more to do with "getting a feel for" the data. After reading [Yelland's *Bayesian Forecasting of Parts Demand* ](https://www.sciencedirect.com/science/article/pii/S0169207009001770?casa_token=WKwx9lufKc4AAAAA:kEO3S0jjCzmsVLZa8arVvXiatZEDVS9MJcSA4U2cspr7vnqpeTyQttf1bs78hTCOHaRZoxRi3A), I was inspired to narrow down my ambition idea to build a model to forecast demand from all merchants for all SKU's. Hence, I want to do some more particular exploratory data analysis on the top 50 SKU's by volume and by frequency, and try to build a model off of these data.

### Loading the Full Data
```{r, include=TRUE}
url <- "https://raw.githubusercontent.com/carsonslater/mentored_research2022/Main/new_baskets_full.csv"

full <- read.csv(url)
```

### Tidying the Data
```{r}
# Finding Percentage of the Missing Data for each column.
colMeans(is.na(full))*100
# There is a very small proportion of missing data in these data 

full <- full %>% mutate(id = as.factor(id), 
                        order_id = as.factor(order_id), 
                        merchant_id = as.factor(merchant_id), 
                        sku_id = as.factor(sku_id), 
                        top_cat_id = as.factor(top_cat_id), 
                        sub_cat_id = as.factor(sub_cat_id))

# Cleaning data so that R can read the time stamp
full$placed_at = substr(full$placed_at, 1, nchar(full$placed_at)-4)

# Changing the placed_by into a POSIXct variable type
full$placed_at = as.POSIXct(full$placed_at)

# Finding NA's
full %>% filter(is.na(full$top_cat_id))

# Removing the 11 NA's
full <- full %>% filter(!is.na(full$top_cat_id))


# Creating more columns with more date-specific information
full <- full %>% mutate(year = format(full$placed_at, "%Y"),
                        month = format(full$placed_at, "%m"),
                        day = format(full$placed_at, "%d"),
                        hour = format(full$placed_at, "%H"),
                        minute = format(full$placed_at, "%M"),
                        second = format(full$placed_at, "%S"),
                        yday = yday(full$placed_at),
                        wday = wday(full$placed_at),
                        yweek = week(full$placed_at))

# Creating factor variables for dates and times
full <- full %>% mutate(year = as.factor(year),
                        month = as.factor(month),
                        day = as.factor(day),
                        hour = as.factor(hour),
                        minute = as.factor(minute),
                        second = as.factor(second),
                        yday = as.factor(yday),
                        wday = as.factor(wday),
                        yweek = as.factor(yweek))

# Looking for duplicates orders
dupes <- get_dupes(full, order_id, placed_at, merchant_id, sku_id)

# Removing the 178 duplicate observations
full <- full %>% distinct(order_id, placed_at, merchant_id, sku_id, .keep_all = TRUE)
```

### Finding the Top 50 SKU's by Total Volume
```{r}
# Collecting the top 50 SKU's
qty_grp <- full %>% group_by(sku_id) %>% 
  summarize(tot_qty = sum(qty)) %>% 
  arrange(desc(tot_qty)) %>% 
  slice(1:50)

# Creating a new data frame with only the top 50 SKU's by volume
full_50 <- full %>% filter(full$sku_id %in% qty_grp$sku_id)

head(full_50)

# Checking if there are only 50 unique SKU's in full_50
full_50 %>% summarize(count = n_distinct(sku_id))
```

We have just found the top 50 `sku_id`'s by total volume. What if we found the top 50 `sku_id`'s by average order volume? This might be slightly more relevant for builing a forecasting model.

### Finding the Top 50 SKU's by Average Order Volume
```{r}
# Collecting the top 50 SKU's
sku_cnt <- full %>% count(sku_id) %>% filter(n >= 20)
  
avg_qty_grp <- full %>% filter(full$sku_id %in% sku_cnt$sku_id) %>% 
  group_by(sku_id) %>% 
  summarize(avg_qty = mean(qty)) %>% 
  arrange(desc(avg_qty)) %>% 
  slice(1:50)
  
# Creating a new data frame with only the top 50 SKU's by average order volume
full_50_avg <- full %>% filter(full$sku_id %in% avg_qty_grp$sku_id)

head(full_50_avg)

# Checking if there are only 50 unique SKU's in full_50_avg
full_50_avg %>% summarize(count = n_distinct(sku_id))

# Finding the common `sku_id`'s between the two subsets.
common <- intersect(full_50$sku_id, full_50_avg$sku_id)
common
```

*It is important to note that in order to make the average order volume subset of these data, I only considered the SKU's that were purchased more than 20 times throughout the period of time that the data covers.*

#### Which subset of the data should I use to build the model?

We need to explore the dynamics of each subset to decipher which one would be most helpful for our purposes. Right off the bat, it appears that the total volume subset `full_50`, over six times the amount of information as the average order volume subset, `full_50_avg`. This might give the former a leg up on which one we decide to use. 

#### Question for further exploration:
 - Which categories are most represented in these two subsets?
 - Which SKU's are the most expensive between the two subsets?

#### `full_50` EDA
```{r}
# For visualization purposes
library(RColorBrewer)
n <- 50
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]

col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, 
                           rownames(qual_col_pals)))

# The color schema I am using
pie(rep(1,n), col=sample(col_vector, n))
```

```{r}
# Number of purchases over time series
full_50 %>%
  ggplot(aes(x = placed_at, color = sku_id)) + 
  geom_freqpoly(binwidth = 86400) +
  scale_colour_manual(values = col_vector)

full_50 %>%
  ggplot(aes(x = placed_at, color = sku_id)) + 
  geom_freqpoly(binwidth = 86400) +
  facet_wrap(~ sku_id) +
  scale_colour_manual(values = col_vector)

# Composition of categories within subset
full_50 %>% 
  ggplot(aes(x = top_cat_id, fill = sub_cat_id)) +
  geom_bar(color = "black") + theme_light() +
  scale_fill_manual(values = col_vector)
```

#### `full_50_avg` EDA
```{r}
# Number of purchases over time series
full_50_avg %>%
  ggplot(aes(x = placed_at, color = sku_id)) + 
  geom_freqpoly(binwidth = 86400) +
  scale_colour_manual(values = col_vector)

full_50_avg %>%
  ggplot(aes(x = placed_at, color = sku_id)) + 
  geom_freqpoly(binwidth = 86400) +
  facet_wrap(~ sku_id) +
  scale_colour_manual(values = col_vector)

# Composition of categories within subset
full_50_avg %>% 
  ggplot(aes(x = top_cat_id, fill = sub_cat_id)) +
  geom_bar(color = "black") + theme_light() +
  scale_fill_manual(values = col_vector)
```

After examining the composition of both of these subsets, we have decided to not consider the average order volume set. What we will  examine for good measure, however is the subset of the full data the contains order information for the top 50 median order volumes. From there we will decide which data set we will want to consider for model building.

### Creating the Median Order Volume Subset
```{r}
med_qty_grp <- full %>% filter(full$sku_id %in% sku_cnt$sku_id) %>% 
  group_by(sku_id) %>% 
  summarize(med_qty = median(qty)) %>% 
  arrange(desc(med_qty)) %>% 
  slice(1:50)

# Creating a new data frame with only the top 50 SKU's by median order volume
full_50_med <- full %>% filter(full$sku_id %in% med_qty_grp$sku_id)

head(full_50_med)

# Checking if there are only 50 unique SKU's in full_50_avg
full_50_med %>% summarize(count = n_distinct(sku_id))

# Finding the common `sku_id`'s between the two subsets.
common2 <- intersect(full_50$sku_id, full_50_avg$sku_id)
common2

common3 <- intersect(full_50_med$sku_id, full_50_avg$sku_id)
common3
```

#### `full_50_med` EDA
```{r}
# Number of purchases over time series
full_50_med %>%
  ggplot(aes(x = placed_at, color = sku_id)) + 
  geom_freqpoly(binwidth = 86400) +
  scale_colour_manual(values = col_vector)

# Composition of categories within subset
full_50_med %>% 
  ggplot(aes(x = top_cat_id, fill = sub_cat_id)) +
  geom_bar(color = "black") + theme_light() +
  scale_fill_manual(values = col_vector)
```

Because `top_cat_id` 4 comprises most of the subset data for `full_50_avg` and `full_50_med`, intuition tells us that `top_cat_id` 4 is a type of good that is bought and sold at a very high unit rate. Although this is useful modeling information, we want our model to be able to predict different types of goods and baskets, so we ought to consider builing our model on `full_50`.

By using `full_50`, we lose information on most of the SKU's, but the trade off is that our model will be trained with less noise to more precisely forecast the most popular SKU's.




 