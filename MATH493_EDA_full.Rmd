---
title: "SKU EDA (Full Data)"
author: "Carson Slater"
date: '2022-09-09'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# Loading packages
library(tidymodels)
library(stringr)
library(janitor)
library(glmnet)
library(lubridate)
library(knitr)
library(mosaic)
library(doParallel)
```

```{r, include=FALSE}
# For parallel processing
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
```

### The Background

This is the second .Rmd file dedicated to EDA for my poster project in the Fall of 2022 aimed at forecasting demand for particular FMCS data. FMCG's are fast-moving, consumer goods, meaning that lots of research can be done here that is market basket analysis! Basket is a consumer's purchases at a given time.

Unfortunately, this is not a market basket analysis project. Here are some of the questions that I have asked, some of which I have tapped into, but others I have yet to explore:

 - What are the busiest times of the year to make note of?
 - What kinds of goods are sold the most frequently?
 - What kinds of goods are sold at higher quantities?
 - What are the average quantities sold for each SKU?
 - Are any of the types of goods sold at higher rates during certain parts of the year?
 - Can we detect any automated purchasing?
 - How many total merchants is this distribution center (DC) servicing?
 - How frequent do these merchants purchase from this DC?
 - Were these merchants customers throughout the entire time these data were collected?
 - What did these merchants purchase? How much?
 
### New Questions I have asked.

The questions listed above are less pin-pointed and have more to do with "getting a feel for" the data. After reading [Yelland's *Bayesian Forecasting of Parts Demand* ](https://www.sciencedirect.com/science/article/pii/S0169207009001770?casa_token=WKwx9lufKc4AAAAA:kEO3S0jjCzmsVLZa8arVvXiatZEDVS9MJcSA4U2cspr7vnqpeTyQttf1bs78hTCOHaRZoxRi3A), I was inspired to narrow down my ambition idea to build a model to forecast demand from all merchants for all SKU's. Hence, I want to do some more particular exploratory data analysis on the top 50 SKU's by volume and by frequency, and try to build a model off of these data.

### Loading the Full Data
```{r,}
url <- "https://raw.githubusercontent.com/carsonslater/mentored_research2022/Main/new_baskets_full.csv"

full <- read.csv(url)
```

### Tidying the Data
```{r}
# Finding Percentage of the Missing Data for each column.
colMeans(is.na(full))*100
# There is a very small proportion of missing data in these data 

full <- full %>% mutate(id = as.factor(id), 
                        order_id = as.factor(order_id), 
                        merchant_id = as.factor(merchant_id), 
                        sku_id = as.factor(sku_id), 
                        top_cat_id = as.factor(top_cat_id), 
                        sub_cat_id = as.factor(sub_cat_id))

# Cleaning data so that R can read the time stamp
full$placed_at = substr(full$placed_at, 1, nchar(full$placed_at)-4)

# Changing the placed_by into a POSIXct variable type
full$placed_at = as.POSIXct(full$placed_at)

# Finding NA's
full %>% filter(is.na(full$top_cat_id))

# Removing the 11 NA's
full <- full %>% filter(!is.na(full$top_cat_id))


# Creating more columns with more date-specific information
full <- full %>% mutate(year = format(full$placed_at, "%Y"),
                        month = format(full$placed_at, "%m"),
                        day = format(full$placed_at, "%d"),
                        hour = format(full$placed_at, "%H"),
                        minute = format(full$placed_at, "%M"),
                        second = format(full$placed_at, "%S"),
                        yday = yday(full$placed_at),
                        wday = wday(full$placed_at),
                        yweek = week(full$placed_at))

# Creating factor variables for dates and times
full <- full %>% mutate(year = as.factor(year),
                        month = as.factor(month),
                        day = as.factor(day),
                        hour = as.factor(hour),
                        minute = as.factor(minute),
                        second = as.factor(second),
                        yday = as.factor(yday),
                        wday = as.factor(wday),
                        yweek = as.factor(yweek))

# Looking for duplicates orders
dupes <- get_dupes(full, order_id, placed_at, merchant_id, sku_id)

# Removing the 178 duplicate observations
full <- full %>% distinct(order_id, placed_at, merchant_id, sku_id, .keep_all = TRUE)
```

### Finding the Top 50 SKU's by Total Volume
```{r}
# Collecting the top 50 SKU's
qty_grp <- full %>% group_by(sku_id) %>% 
  summarize(tot_qty = sum(qty)) %>% 
  arrange(desc(tot_qty)) %>% 
  slice(1:50)

# Creating a new data frame with only the top 50 SKU's by volume
full_50 <- full %>% filter(full$sku_id %in% qty_grp$sku_id)

head(full_50)

# Checking if there are only 50 unique SKU's in full_50
full_50 %>% summarize(count = n_distinct(sku_id))
```

We have just found the top 50 `sku_id`'s by total volume. What if we found the top 50 `sku_id`'s by average order volume? This might be slightly more relevant for builing a forecasting model.

### Finding the Top 50 SKU's by Average Order Volume
```{r}
# Collecting the top 50 SKU's
sku_cnt <- full %>% count(sku_id) %>% filter(n >= 20)
  
avg_qty_grp <- full %>% filter(full$sku_id %in% sku_cnt$sku_id) %>% 
  group_by(sku_id) %>% 
  summarize(avg_qty = mean(qty)) %>% 
  arrange(desc(avg_qty)) %>% 
  slice(1:50)
  
# Creating a new data frame with only the top 50 SKU's by average order volume
full_50_avg <- full %>% filter(full$sku_id %in% avg_qty_grp$sku_id)

head(full_50_avg)

# Checking if there are only 50 unique SKU's in full_50_avg
full_50_avg %>% summarize(count = n_distinct(sku_id))
```

#### Which subset of the data should I use to build the model?

We need to explore the dynamics of each subset to decipher which one would be most helpful for our purposes. Right off the bat, it appears that the total volume subset `full_50`, over six times the amount of information as the average order volume subset, `full_50_avg`. This might give the former a leg up on which one we decide to use. 

#### `full_50` EDA
```{r}
# For visualization purposes
library(RColorBrewer)
n <- 50
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]

col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, 
                           rownames(qual_col_pals)))

pie(rep(1,n), col=sample(col_vector, n))
```

```{r}
full_50 %>%
  ggplot(aes(x = placed_at, color = sku_id)) + 
  geom_freqpoly(binwidth = 86400)

full_50 %>%
  ggplot(aes(x = placed_at, color = sku_id)) + 
  geom_freqpoly(binwidth = 86400) +
  facet_wrap(~ sku_id)
```

#### Question for further exploration:
 - Which categories are most represented in these two subsets?
 